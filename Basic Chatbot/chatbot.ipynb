# Importing the required Libraries

import numpy as np
import nltk
import string
import random

# Importing and reading corpus

f = open('SampleData.txt', 'r', errors = 'ignore')
raw_data= f.read()
raw_data=raw_data.lower()
nltk.download('punkt')
nltk.download('wordnet')
sent_tokens = nltk.sent_tokenize(raw_data)
word_tokens = nltk.word_tokenize(raw_data)

# Text preprocessing

lemmer = nltk.stem.WordNetLemmatizer()
def LemTokens(tokens):
  return [lemmer.lemmatize(token) for token in tokens]
remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)
def LemNormalize(text):
  return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))

# Greeting Function  

greet_inputs = ('hi','hello','whatsup','how are you')
greet_outputs = ['Hello!','I am gald u asked, I am fine','what about you']
def greet(sentence):
  for word in sentence.split():
    if word.lower() in greet_inputs:
      return random.choice(greet_outputs)  

# Response Generation

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

def response(user_response):
  robo1_response=''
  TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')
  tfidf=TfidfVec.fit_transform(sent_tokens)
  vals = cosine_similarity(tfidf[-1],tfidf)
  idx = vals.argsort()[0][-2]
  flat = vals.flatten()
  flat.sort()
  req_tfidf = flat[-2]
  if req_tfidf==0:
    robo1_response+="I am sorry! I don't understand you."
    return robo1_response
  else:
    robo1_response+=sent_tokens[idx]
    return robo1_response

flag=True
print("BOT: My name is Chitti, tell me what is ur question. If u want to quit at any point of time Please enter Bye!")
while flag:
  user_response=input().lower()
  if user_response!="bye":
    if user_response=="thanks" or user_response=="thank you":
      flag=False
      print("BOT: You'r welcome.")
    else:
      if (greet(user_response)!=None):
        print("BOT: "+greet(user_response))
      else:
        sent_tokens.append(user_response)
        word_tokens=word_tokens+nltk.word_tokenize(user_response)
        final_words=list(set(word_tokens))
        print("BOT: ",end='')
        print(response(user_response))
        sent_tokens.remove(user_response)
  else:
    flag=False
    print("BOT: good bye!")  
  
        
        
